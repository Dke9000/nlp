{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "full_data = pd.read_csv('input/full_data_final.csv', index_col=0)\n",
    "\n",
    "print(full_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libriries and splitting the full data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Data valid/train split\n",
    "train_texts, valid_text, y_train, y_valid = train_test_split(\n",
    "    full_data['content'], full_data['sarcastic'], random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the effect of log length of sarcastic/not sarcastic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data analysis\n",
    "full_data.loc[full_data['sarcastic'] == 1, 'content'].str.len().apply(np.log1p).hist(label='sarcastic', alpha=.5)\n",
    "full_data.loc[full_data['sarcastic'] == 0, 'content'].str.len().apply(np.log1p).hist(label='normal', alpha=.5)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying wordcloud for both sarcastic/not sarcastic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Word cloud\n",
    "wordcloud = WordCloud(background_color='white', stopwords = STOPWORDS,\n",
    "                max_words = 200, max_font_size = 100, \n",
    "                random_state = 17, width=800, height=400)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "wordcloud.generate(str(full_data.loc[full_data['sarcastic'] == 1, 'content']))\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "wordcloud.generate(str(full_data.loc[full_data['sarcastic'] == 0, 'content']))\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the pipeline (transformer + model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_texts))\n",
    "\n",
    "# Model pipeline creation\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3), max_features=500000, analyzer='word', max_df=0.8)\n",
    "\n",
    "logit = LogisticRegression(C=1, n_jobs=-1, solver='lbfgs', random_state=0, verbose=1, penalty='l2')\n",
    "\n",
    "tfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), ('logit', logit)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the data to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "tfidf_logit_pipeline.fit(train_texts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "valid_pred = tfidf_logit_pipeline.predict(valid_text)\n",
    "\n",
    "print(accuracy_score(y_valid, valid_pred))\n",
    "print(classification_report(y_valid, valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to plot a pretty confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conf matrix function\n",
    "def plot_confusion_matrix(actual, predicted, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix', figsize=(7,7),\n",
    "                          cmap=plt.cm.Blues, path_to_save_fig=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(actual, predicted).T\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Predicted label')\n",
    "    plt.xlabel('True label')\n",
    "    \n",
    "    if path_to_save_fig:\n",
    "        plt.savefig(path_to_save_fig, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_valid, valid_pred, \n",
    "                      tfidf_logit_pipeline.named_steps['logit'].classes_, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ELI5 library to outline important phrases for sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "\n",
    "# Sarcasm phrases detection\n",
    "eli5.show_weights(estimator=tfidf_logit_pipeline.named_steps['logit'],\n",
    "                  vec=tfidf_logit_pipeline.named_steps['tf_idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
